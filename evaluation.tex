\section{Evaluation}
\label{sec:evaluation}
Jason
%3.5 pages 
% explain that elasticsearch is one way to support these queries


% we need to map queries to elasticsearch
% a. time to preprocess (sparql none / ld fragments a little / ld joins a lot
% b. time to load (sparql some / ld fragments none / ld joins some
% c. time to execute queries (sparql lots / ld fragments infinity / ld joins litte )
% d. benchmark explanation

For our evaluation, we designed a benchmark to simulate some number of users iteratively refining searches for evidence of human trafficking by exploring connections between web pages containing ads and the entities extract from them, namely phone numbers, emails, adult services, sellers, and offers, against databases serving the knowledge graph.
The benchmark measured query response time under varying levels of load.
Query response time was also subject to meeting quality of service measures outlined in the Motivation section and driven by \cite{nielsen1994usability}.
Although query result ranking is one of the most compelling metrics to measure for an application like this, it was not evaluated in this paper, because most triple stores do not have an inherent way to rank the quality of keyword search results based on relevance.

\subsection{Database Considerations}
To test our benchmark, we chose Virtuoso 7 as our triple store and Elasticsearch as our search engine, so we could compare SPARQL performance against an implementation of LD-VIEWS on a NoSQL database.  
There are many alternatives available.  

We did not make an attempt to automatically translate queries from SPARQL like the examples in the motivation to Elasticsearch's query and aggregation API.  
Instead, we wrote a specification and interpreter to compose the elements of the queries as necessary and apply filters as searches are iteratively refined in the benchmark.  
The property paths in SPARQL became document field names in Elasticsearch.  
The SPARQL group by and aggregation methods became Elasticsearch aggs. 

In addition, to avoid unfairly penalizing Virtuoso because the SPARQL standard only has support for regular expression matching to handle keyword searches, the SPARQL queries for the benchmark are written using the Virtuoso specific bif:contains function.
This enables Virtuoso to take advantage of its text index.  
Without it, the evaluation of even simple regular expression queries is incredibly expensive.  


\subsection{Benchmark Machines}
For our evaluation we used the Virtuoso 7.2 Enterprise PAGO Edition\cite{virtuosopago} available on AWS on r3.2xlarge which has 8 CPUs and 61 GB of RAM and RHEL 7 installed.  
The data was stored on EBS volumes backed by SSDs with the maximum provisioned IOPs available. 
We also ran Elasticsearch using CentOS with the same instance type.  
For our query driver, we used an Amazon Linux AMI 2015-09.2 on an instance type of m4.xlarge to ensure we had the best networking performance possible in the us-west-2 region.    
For Virtuoso, we reused the configuration published by the Berlin SPARQL Benchmark V7 \cite{bsbmv7}.
For Elasticsearch we made no configuration changes except to increase the size of thread pools so that new queries were not rejected when the pending search queue was full. 

\subsection{Benchmark Execution}
User arrival is modeled as a Poisson process with an arrival rate that follows a configurable exponential distribution.  
In our evaluation, the mean arrival time is varied to simulate different levels of query load.  
We vary the mean arrival time from one user every ten seconds (a light load) to one user every second (a medium load) to ten users every one second (a heavy load).
The benchmark also allows for artificially limiting the number of concurrent users searching the knowledge graph so that we can explore their interactive effects without overwhelming the server.  
We run the benchmark with at three concurrency levels: a single user, at most ten users, and at most one hundred users.
If a user arrives when the maximum level of concurrent users has been reached, the user will be placed in a queue until a slot is available.  
We always run the benchmark with 100 users attempting to query the database over the course of the benchmark.

Each user will complete five different searches. 
The user first selects an entity type to search, for example, web pages.
As the search results come back, the user will iteratively refine their search based on the results.
After each result, the user has a probability alpha, set to 0.1, that their search is satisfied, after which they won't search any more.
The user will not refine their search more than three times.
If the search results are empty, the user will give up on the search and move on to their next search.

Each search starts by the user selecting two keywords.  
These keywords are selected from a list of the top 2,000 most common words found in the content of the crawled web pages that go in to the knowledge graph, minus stop words. 
The database will then return 20 web pages related to those keywords, and gather the relevant information from the RDF graph neighborhood surrounding them, including any offers made on the web page, any information gathered about the seller, including extracted phone numbers and emails, etc.  

When the keyword search is issued, a series of separate facet queries appropriate for web pages are issued against the all web pages related to those keywords in parallel.
To meet the quality of service standards, the search query and the facet queries should return with 1 second.

When the keyword search returns, a series of anchored queries based on the 20 web pages will also be issued to populate the appropriate visualizations. 
To meet the quality of service standards, search query, facet queries, and the anchored queries should all return with 10 seconds.  
If any query takes longer than 120 seconds, the benchmark will timeout they query and record the response time as 120 seconds.  

After all the queries have returned, the user waits some time based on an exponential distribution, with a mean of 3 seconds, to simulate the user making a decision, and selects one of the facet values to refine their search with.
The user then continues to iterate over their search until they are satisfied.  

% intuition
% how long does it take to do a keyword search
% how long does it take to facet
% how long does it take to assemble an entity
% how long does it take display results using a visualization
\subsection{Benchmark Response Times}
\subsubsection{Single User Interaction}
 \begin{table}
 \tiny{
    \begin{tabular}{ c c c c c }
        Database & Keyword & Facet & Facet (Missing) & Click Search & Anchored & Click Viz \\ 
        Virtuoso 7 & 439 & 1039 & 1008 & 1725 & 206 &  1790 \\ 
        Virtuoso 7 + nginx (5) & 161 & 321 & 240 & 658 & 29 &  673 \\ 
        Virtuoso 7 Cluster (5) & 28923 & 25880 & 28346 & 30503 & 4539 & 35650\\
        ES 1.7.3 Standalone & 93 & 77 & 76 & 111 & 1356 & 1531 \\ 
        ES 1.7.3 Cluster (5) & 41 & 22 & 21 & 60 & 84 & 156 \\ 
    \end{tabular} 
    }
    \caption{Avg. Query Times in Milliseconds by Database and Query Type For Single User Query Load for 200 million triples}
    \label{table:qt_single_user_200m}
    
\end{table}
We began the evaluation by throttling the workload so that the hundred simulated users' access to the application was limited to one at a time.
In this set up, Elasticsearch beats Virtuoso across the board, with the exception of the Anchored queries as shown in Table~\ref{table:qt_single_user_200m}.  
The Anchored queries do not require free text search nor aggregation over a large amount of data, so Virtuoso should win because it only has to expand a a small amount of the graph from a single subject URI.
A single Elasticsearch instance must query every shard and aggregate the results, but when the work is split over many machines, there is much speed up from distributing the workload and lower resource usage pressure.   
With only two hundred million triples, Virtuoso has the additional benefit that the graph and its indexes easily fit inside RAM.
Ultimately, although the Anchored queries took longer individually, since they were issued in parallel, Elasticsearch was able to keep up.
The Virtuoso Cluster performed poorly for a single user and was not evaluated further. 
This seems to confirm the intuition that the Virtuoso Cluster Edition performs poorly when serving many queries in parallel as discussed at the end of this paper \cite{harthlinked}.  
In this application, a single user's click can produce over a hundred parallel queries against the database. 
Every other set up was able to achieve the minimum performance under the user interface guidelines. 
The five node Elasticsearch cluster performed the best, outperforming five Virtuoso nodes serving queries issued in round robin from an nginx server by an order of magnitude and almost two orders of magnitude over a single Virtuoso server on some queries. 

\begin{table} 
\tiny{
    \begin{tabular}{ c c c c c }
        Database & Keyword & Facet & Facet (Missing) & Click Search & Anchored & Click Viz \\ 
        Virtuoso 7 & 4649 & 13368 & 12402 & 25891 & 6778 &  27452 \\ 
        ES 1.7.3 Standalone & 70 & 80 & 79 & 124 & 3565 & 3768 \\ 
        ES 1.7.3 Cluster (5) & 469 & 366 & 363 & 496 & 850 & 1415 \\ 
        ES 1.7.3 Cluster (20) & 108 & 105 & 106 & 148 & 1413 & 1510 \\ 
    \end{tabular} 
    }
    \caption{Avg. Query Times in Milliseconds by Database and Query Type For Single User Query Load for 1.2 billion triples}
    \label{table:qt_single_user_1b}
\end{table}

We continued the evaluation for a single user with a larger graph on the order of 1 billion triples.  
The graph and indexes were large enough that they no longer fit in working memory for either Elasticsearch nor Virtuoso on a single machine.
Virtuoso was not able to achieve the required response times of 1 second for Click to Search and 10 seconds for Click to Visualization as shown in Table ~\ref{table:qt_single_user_1b}.
A standalone Elasticsearch machine was able to achieve the necessary user interface performance guidelines for a single user, and Elasticsearch with 5 and 20 nodes was easily able to provide the quality of service required.
It is not immediately clear why search is faster on a standalone cluster.
It may be that merging results across multiple machines has a significant impact that is not ameliorated until enough servers can process them in parallel. 
The 20 node cluster was able to serve the search and facet results over 100x faster than a single Virtuoso 7 instance. 
The next step in our evaluation was allowing multiple users to use the application at the same time.
We did not continue the evaluation for parallel work loads for Virtuoso because it could not meet the requirements for a single user.

