\section{Evaluation}
\label{sec:evaluation}
Jason
%3.5 pages 
% explain that elasticsearch is one way to support these queries


% we need to map queries to elasticsearch
% a. time to preprocess (sparql none / ld fragments a little / ld joins a lot
% b. time to load (sparql some / ld fragments none / ld joins some
% c. time to execute queries (sparql lots / ld fragments infinity / ld joins litte )
% d. benchmark explanation

For our evaluation, we designed a benchmark to simulate some number of users iteratively refining searches for evidence of human trafficking by exploring connections between web pages containing ads and the entities extract from them, namely phone numbers, emails, adult services, sellers, and offers, against databases serving the knowledge graph.
The benchmark measured query response time under varying levels of load.
Query response time was also subject to meeting quality of service measures outlined in the Motivation section and driven by \cite{nielsen1994usability}.
Although query result ranking is one of the most compelling metrics to measure for an application like this, it was not evaluated in this paper, because most triple stores do not have an inherent way to rank the quality of keyword search results based on relevance.

\subsection{Database Considerations}
To test our benchmark, we chose Virtuoso 7 as our triple store and Elasticsearch as our search engine, so we could compare SPARQL performance against an implementation of LD-VIEWS on a NoSQL database.  
There are many alternatives available.  

We did not make an attempt to automatically translate queries from SPARQL like the examples in the motivation to Elasticsearch's query and aggregation API.  
Instead, we wrote a specification and interpreter to compose the elements of the queries as necessary and apply filters as searches are iteratively refined in the benchmark.  
The property paths in SPARQL became document field names in Elasticsearch.  
The SPARQL group by and aggregation methods became Elasticsearch aggs. 

In addition, to avoid unfairly penalizing Virtuoso because the SPARQL standard only has support for regular expression matching to handle keyword searches, the SPARQL queries for the benchmark are written using the Virtuoso specific bif:contains function.
This enables Virtuoso to take advantage of its text index.  
Without it, the evaluation of even simple regular expression queries is incredibly expensive.  


\subection{Benchmark Machines}
For our evaluation we used the Virtuoso 7.2 Enterprise PAGO Edition\cite{virtuosopago} available on AWS on r3.2xlarge which has 8 CPUs and 61 GB of RAM and RHEL 7 installed.  
The data was stored on EBS volumes backed by SSDs with the maximum provisioned IOPs available. 
We also ran Elasticsearch using CentOS with the same instance type.  
For our query driver, we used an Amazon Linux AMI 2015-09.2 on an instance type of m4.xlarge to ensure we had the best networking performance possible in the us-west-2 region.    
For Virtuoso, we reused the configuration published by the Berlin SPARQL Benchmark V7 \cite{bsbmv7}.
For Elasticsearch we made no configuration changes except to increase the size of thread pools so that new queries were not rejected when the pending search queue was full. 

\subsection{Benchmark Execution}
User arrival is modeled as a Poisson process with an arrival rate that follows a configurable exponential distribution.  
In our evaluation, the mean arrival time is varied to simulate different levels of query load.  
We vary the mean arrival time from one user every ten seconds (a light load) to one user every second (a medium load) to ten users every one second (a heavy load).
The benchmark also allows for artificially limiting the number of concurrent users searching the knowledge graph so that we can explore their interactive effects without overwhelming the server.  
We run the benchmark with at three concurrency levels: a single user, at most ten users, and at most one hundred users.
If a user arrives when the maximum level of concurrent users has been reached, the user will be placed in a queue until a slot is available.  
We always run the benchmark with 100 users attempting to query the database over the course of the benchmark.

Each user will complete five different searches. 
The user first selects an entity type to search, for example, web pages.
As the search results come back, the user will iteratively refine their search based on the results.
After each result, the user has a probability alpha, set to 0.1, that their search is satisfied, after which they won't search any more.
The user will not refine their search more than three times.
If the search results are empty, the user will give up on the search and move on to their next search.

Each search starts by the user selecting two keywords.  
These keywords are selected from a list of the top 2,000 most common words found in the content of the crawled web pages that go in to the knowledge graph, minus stop words. 
The database will then return 20 web pages related to those keywords, and gather the relevant information from the RDF graph neighborhood surrounding them, including any offers made on the web page, any information gathered about the seller, including extracted phone numbers and emails, etc.  

When the keyword search is issued, a series of separate facet queries appropriate for web pages are issued against the all web pages related to those keywords in parallel.
To meet the quality of service standards, the search query and the facet queries should return with 1 second.

When the keyword search returns, a series of anchored queries based on the 20 web pages will also be issued to populate the appropriate visualizations. 
To meet the quality of service standards, search query, facet queries, and the anchored queries should all return with 10 seconds.  

After all the queries have returned, the user waits some time based on an exponential distribution, with a mean of 3 seconds, to simulate the user making a decision, and selects one of the facet values to refine their search with.
The user then continues to iterate over their search until they are satisfied.  




% intuition
% how long does it take to do a keyword search
% how long does it take to facet
% how long does it take to assemble an entity
% how long does it take display results using a visualization
\subsection{Benchmark Response Times}

 \begin{table} 
    \begin{tabular}{ c c c c c }
        Database & Keyword & Facet & Facet (Missing) & Click Search & Anchored & Click Viz \\ 
        Virtuoso 7 & 439 & 1039 & 1008 & 1725 & 206 &  1790 \\ 
        Virtuoso 7 + nginx & 161 & 321 & 240 & 658 & 29 &  673 \\ 
        Virtuoso 7 Cluster & 28923 & 25880 & 28346 & 30503 & 4539 & 35650\\
        ES 1.7.3 Standalone & 93 & 77 & 76 & 111 & 1356 & 1351 \\ 
        ES 1.7.3 Cluster (5) & 41 & 22 & 21 & 60 & 84 & 156 \\ 
    \end{tabular} 
    \caption{Avg. Query Times in Milliseconds by Database and Query Type For Single User Query Load for 200 million triples}
    \label{table:qt_single_user_200m}
\end{table}

\begin{table} 
    \begin{tabular}{ c c c c c }
        Database & Keyword & Facet & Facet (Missing) & Click Search & Anchored & Click Viz \\ 
        Virtuoso 7 & 4649 & 13368 & 12402 & 25891 & 6778 &  27452 \\ 
        ES 1.7.3 Standalone & 0 & 0 & 0 & 0 & 0 & 0 \\ 
        ES 1.7.3 Cluster (5) & 469 & 366 & 363 & 496 & 850 & 1415 \\ 
        ES 1.7.3 Cluster (20) & 108 & 105 & 106 & 148 & 1413 & 1510 \\ 
    \end{tabular} 
    \caption{Avg. Query Times in Milliseconds by Database and Query Type For Single User Query Load for 1.2 billion triples}
    \label{table:qt_single_user_1b}
\end{table}
